{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4786295",
        "outputId": "7d77b496-e8ab-4737-92a7-b59b1b41c19a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 13 09:40:00 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    33W /  70W |   1123MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "## COMPROBAR GPU ASIGNADA EN COLABORATORY\n",
        "#########################################################################\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "s1QxgGVZ_PvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d98498ba-3cd6-485b-d27f-7589319b9251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "## MONTAR DRIVE\n",
        "#########################################################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "a6cef0d7"
      },
      "outputs": [],
      "source": [
        "#########################################################################\n",
        "## LIBRERIAS NECESARIAS\n",
        "#########################################################################\n",
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from tensorflow.keras.layers import concatenate\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import layers\n",
        "import copy\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sys\n",
        "\n",
        "import os\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "abc775a7"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "## Variables globales\n",
        "###########################################################################\n",
        "BATCH_SIZE=32\n",
        "EPOCHS=50\n",
        "EPISODES=10\n",
        "EPISODES_EVALUATE_G=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85cff0c4"
      },
      "source": [
        "# Gym CartPole-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39e0a1c1"
      },
      "source": [
        "Un péndulo está unido mediante una articulación no accionada a un carro que se desplaza a lo largo de una pista sin fricción. El péndulo se coloca verticalmente sobre el carro y el objetivo es equilibrar el poste aplicando fuerzas en dirección izquierda y derecha sobre el carro.\n",
        "\n",
        "**Espacio de Acciones**: Espacio discreto de tamaño (2)\n",
        " * Acción 0: Empujar el carro hacia la izquierda\n",
        " * Acción 1: Empujar el carro hacia la derecha\n",
        "\n",
        "    \n",
        "   \n",
        "**Espacio de Observaciones**: Espacio discreto de tamaño (4,)\n",
        "\n",
        "* La observación es un ndarray con forma (4,) con los valores correspondientes a las siguientes posiciones y velocidades:\n",
        "         Num    Observación                         Min                            Max\n",
        "\n",
        "          0    Posición del Carro                 - 4.8                            4.8\n",
        "\n",
        "          1    Velocidad del Carro                 -Inf                            Inf\n",
        "\n",
        "          2    Ángulo del Poste              ~ -0.418 rad (-24°)          ~ 0.418 rad (24°)\n",
        "\n",
        "          3    Velocidad Angular del Poste         -Inf                            Inf\n",
        "\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82e0467a",
        "outputId": "42e712ca-5f1f-4198-b9dc-63b2cb0c3175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "# Definimos el entorno\n",
        "env= gym.make('CartPole-v1')\n",
        "\n",
        "# Obtenemos el espacio de estados y acciones del entorno\n",
        "ob_space=env.observation_space\n",
        "\n",
        "# Mostramos el número de acciones del entorno\n",
        "print(env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9caa49f2"
      },
      "source": [
        "\n",
        "\n",
        "# Discriminador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kshaCKl6UIJb"
      },
      "source": [
        "## Red neuronal del Discriminador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d9e6597",
        "outputId": "85182774-63db-4a98-8ad3-6e7152cb2c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer1 (Dense)              (None, None, 20)          140       \n",
            "                                                                 \n",
            " layer2 (Dense)              (None, None, 20)          420       \n",
            "                                                                 \n",
            " layer3 (Dense)              (None, None, 20)          420       \n",
            "                                                                 \n",
            " prob (Dense)                (None, None, 1)           21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,001\n",
            "Trainable params: 1,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Red neuronal del Discriminador\n",
        "# Input: listas de longitud 6, [s,a]\n",
        "# Output: probabilidad de real o falso [0,1]\n",
        "discriminator_net=keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(None,ob_space.shape[0]+2)),\n",
        "        layers.Dense(units=20,activation=tf.nn.leaky_relu, name='layer1'),\n",
        "        layers.Dense(units=20,activation=tf.nn.leaky_relu, name='layer2'),\n",
        "        layers.Dense(units=20, activation=tf.nn.leaky_relu, name='layer3'),\n",
        "        layers.Dense(units=1, activation=tf.sigmoid, name='prob'),\n",
        "\n",
        "    ],\n",
        "    name=\"discriminator_net\"\n",
        "\n",
        ")\n",
        "discriminator_net.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMIc-9PoUNdG"
      },
      "source": [
        "## Función de pérdida del Discriminador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "78bdb460"
      },
      "outputs": [],
      "source": [
        "# Función de pérdida del Discriminador\n",
        "# prob1=>output de la red neuronal anterior cuando recibe como entrada [s,a] de la base de datos, REAL\n",
        "# prob2=>output de la red neuronal anterior cuando recibe como entrada [s,a] FALSO\n",
        "def loss_fn_D(prob1, prob2):\n",
        "    # Esperanza del logaritmo de la D(x)=salida de la red neuronal cuando x=entrada REA\n",
        "    loss_expert=tf.reduce_mean(tf.math.log(tf.clip_by_value(prob1,0.01,1)))\n",
        "\n",
        "    # Esperanza del logaritmo de 1-D(x) donde D(x)=salida de la red neuronal cuando x=entrada FALSA\n",
        "    loss_agent=tf.reduce_mean(tf.math.log(tf.clip_by_value(1-prob2,0.01,1)))\n",
        "    loss_expert=tf.cast(loss_expert, dtype=tf.float32)\n",
        "    loss_agent=tf.cast(loss_agent, dtype=tf.float32)\n",
        "    loss=loss_expert+loss_agent\n",
        "    loss=-loss\n",
        "\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_R6wBgyUSYo"
      },
      "source": [
        "## Clase del Discriminador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "5e19555d"
      },
      "outputs": [],
      "source": [
        "# Clase DISCRIMINADOR\n",
        "class Discriminator:\n",
        "    def __init__(self, env, discriminator_net, expert_s, expert_a, agent_s, agent_a):\n",
        "        # -Red neuronal del Discriminador\n",
        "        self.discriminator_net=discriminator_net\n",
        "        # -Experto: [s,a]\n",
        "        self.expert_s=expert_s\n",
        "        self.expert_a=expert_a\n",
        "        expert_a_one_hot=tf.one_hot(self.expert_a,depth=env.action_space.n)\n",
        "        # Añadimos ruido para estabilizar el entrenamiento\n",
        "        expert_a_one_hot+= tf.random.normal(tf.shape(expert_a_one_hot), mean=0.2, stddev=0.1, dtype=tf.float32)/1.2\n",
        "        self.expert_s_a=tf.concat([self.expert_s,expert_a_one_hot],axis=1)\n",
        "\n",
        "        # -Agente:  [s,a]\n",
        "        self.agent_s=agent_s\n",
        "        self.agent_a=agent_a\n",
        "        agent_a_one_hot=tf.one_hot(self.agent_a,depth=env.action_space.n)\n",
        "        agent_a_one_hot+= tf.random.normal(tf.shape(agent_a_one_hot), mean=0.2, stddev=0.1, dtype=tf.float32)/1.2\n",
        "        self.agent_s_a=tf.concat([self.agent_s,agent_a_one_hot],axis=1)\n",
        "\n",
        "\n",
        "\n",
        "        # Calculamos la salida de la red para [s,a] del experto y del agente ya que lo necesitamos para reward\n",
        "\n",
        "        # -Salida de la red neuronal Discriminador para [s,a] expertos(verdaderos)\n",
        "        self.prob_expert=self.discriminator_net(self.expert_s_a)\n",
        "\n",
        "        # -Salida  de la red neuronal Discrimiinador para [s,a] Agente(falsos)\n",
        "        self.prob_agent=self.discriminator_net(self.agent_s_a)\n",
        "\n",
        "        #-Recompensa obtenida cuando el Agente realiza [s,a] falsas\n",
        "        self.rewards=tf.math.log(tf.clip_by_value(self.prob_agent,1e-10,1)) #log(P(expert|s,a)) cuando mas grande es mejor el agente\n",
        "\n",
        "\n",
        "    def getNet(self):\n",
        "        return self.discriminator_net\n",
        "\n",
        "    def getAgent_S_A(self):\n",
        "        return self.agent_s_a\n",
        "\n",
        "    def getExpert_S_A(self):\n",
        "        return self.expert_s_a\n",
        "\n",
        "    def getProb(self):\n",
        "        return self.prob_expert, self.prob_agent\n",
        "\n",
        "    def getRewards(self):\n",
        "        return self.rewards\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f540f7a"
      },
      "source": [
        "# Generador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQRVvHKWUWBU"
      },
      "source": [
        "## Redes neuronales del Generador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce26c866",
        "outputId": "80e6dc8a-2872-41bf-bbe4-45a3c2db325a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"generator_net_Act\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer1 (Dense)              (None, None, 20)          100       \n",
            "                                                                 \n",
            " layer2 (Dense)              (None, None, 20)          420       \n",
            "                                                                 \n",
            " layer3 (Dense)              (None, None, 2)           42        \n",
            "                                                                 \n",
            " layer4 (Dense)              (None, None, 2)           6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 568\n",
            "Trainable params: 568\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Red neuronal del Generador donde se producen acciones\n",
        "# Input: estados, listas de tamaño 4, s=[s0,s1,s2,s3]\n",
        "# Output: acciones, listas de tamaño 2, a=[a0,a1]\n",
        "generator_net_Act=keras.Sequential(\n",
        "    [\n",
        "            keras.Input(shape=(None,ob_space.shape[0])),\n",
        "            layers.Dense(units=20, activation=tf.tanh,name='layer1'),\n",
        "            layers.Dense(units=20, activation=tf.tanh, name='layer2'),\n",
        "            layers.Dense(units=2, activation=tf.tanh, name='layer3'),\n",
        "            layers.Dense(units=2, activation=tf.nn.softmax, name='layer4')\n",
        "\n",
        "        ],\n",
        "    name=\"generator_net_Act\"\n",
        ")\n",
        "\n",
        "generator_net_Act.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "429f8080",
        "outputId": "be076f62-e3e1-43c1-ef52-5383d2554a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"generator_v_preds\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer1 (Dense)              (None, None, 20)          100       \n",
            "                                                                 \n",
            " layer2 (Dense)              (None, None, 20)          420       \n",
            "                                                                 \n",
            " layer3 (Dense)              (None, None, 1)           21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 541\n",
            "Trainable params: 541\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Red neuronal del Generador donde se producen v_pred\n",
        "# Input: estados, listas de tamaño 4, s=[s0,s1,s2,s3]\n",
        "# Output: v_pred, listas de tamaño 1, v_pred\n",
        "\n",
        "generator_net_v_preds=keras.Sequential(\n",
        "    [\n",
        "            keras.Input(shape=(None,ob_space.shape[0])),\n",
        "            layers.Dense(units=20, activation=tf.tanh,name='layer1'),\n",
        "            layers.Dense(units=20, activation=tf.tanh, name='layer2'),\n",
        "            layers.Dense(units=1, activation=None, name='layer3'),\n",
        "        ],\n",
        "    name=\"generator_v_preds\"\n",
        ")\n",
        "\n",
        "generator_net_v_preds.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOU1XlPtUeOG"
      },
      "source": [
        "## Función de pérdida del Generador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "0753302f"
      },
      "outputs": [],
      "source": [
        "# Función de pérdida del Generador\n",
        "def loss_fn_ppo(act_probs, act_probs_old, gaes, clip_value=0.2):\n",
        "    #--> Construir el calculo del grafo para loss_clip\n",
        "\n",
        "    ratios=tf.exp(tf.math.log(tf.clip_by_value(act_probs, 1e-10, 1.0))\n",
        "                    - tf.math.log(tf.clip_by_value(act_probs_old, 1e-10, 1.0)))\n",
        "\n",
        "    clipped_ratios= tf.clip_by_value(ratios, clip_value_min=1 - clip_value, clip_value_max=1 + clip_value)\n",
        "    loss_clip=tf.minimum(tf.multiply(gaes, ratios), tf.multiply(gaes, clipped_ratios))\n",
        "    loss_clip = tf.reduce_mean(loss_clip)\n",
        "\n",
        "    # minimizar -loss == maximizar loss\n",
        "    loss = -loss_clip\n",
        "    tf.summary.scalar('total', loss)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orx8ZvtGUirZ"
      },
      "source": [
        "## Clase del Generador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "ad26b1b9"
      },
      "outputs": [],
      "source": [
        "# Clase del GENERADOR: política con su optimizador PPO\n",
        "# Observesé que cada generador implementa una política distinta, por tanto, se ha decidido llamar a la clase\n",
        "# Policy_net en lugar de generator\n",
        "class Policy_net:\n",
        "    def __init__(self, name: str, env, obs):\n",
        "        \"\"\"\n",
        "        env: gym env\n",
        "        obs:\n",
        "        \"\"\"\n",
        "        # -Entorno\n",
        "        self.env=env\n",
        "\n",
        "        env.reset()\n",
        "\n",
        "        # -El algoritmo de Optimización de Política Proximal, PPO, combina ideas del algoritmo A2C\n",
        "        # (que utiliza múltiples trabajadores) y del algoritmo TRPO (que utiliza una región de confianza para\n",
        "        # mejorar el actor).\n",
        "        self.model=PPO(policy=\"MlpPolicy\", env=env, verbose=0)\n",
        "\n",
        "\n",
        "        self.model.learn(total_timesteps=25)\n",
        "\n",
        "        # -Observación inicial a partir de la cual se crean las acciones iniciales\n",
        "        # haciendo uso de las redes neuronales del generador\n",
        "        self.obs=np.reshape(np.array(obs),(1,4))\n",
        "\n",
        "        # Utilizamos las dos redes neuronales que hemos creado : generator_net_Act y generator_net_v_preds\n",
        "        # V_pred=>recompensa media de que un agente ejecute una acción\n",
        "\n",
        "        # -Acción inicial generada con red neuronal y v_pred con red neuronal\n",
        "        self.act_probs =generator_net_Act(self.obs)\n",
        "        self.v_preds = generator_net_v_preds(self.obs)\n",
        "\n",
        "        # -Accion estocástica inicial\n",
        "        self.act_stochastic = tf.random.categorical(tf.math.log(self.act_probs), num_samples=1)\n",
        "\n",
        "        # -Acción determinística inicial\n",
        "        self.act_deterministic = tf.argmax(self.act_probs, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    # Para cada estado obs me dice la acción que el agente va a ejecutar sobre el entorno junto con v_pred\n",
        "    # La elección de la acción puede ser estocástica o determinística\n",
        "    def act(self, stochastic=True):\n",
        "        if stochastic:\n",
        "            return self.act_stochastic, self.v_preds\n",
        "        else:\n",
        "            return self.act_deterministic, self.v_preds\n",
        "\n",
        "    def get_action_prob(self):\n",
        "        return self.act_probs\n",
        "\n",
        "    def get_v_preds(self):\n",
        "        return self.v_preds\n",
        "\n",
        "    def get_obs(self):\n",
        "        return self.obs\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def get_trainable_variables(self):\n",
        "        return self.model.get_parameters()\n",
        "\n",
        "    # Generar [s,a] falsos\n",
        "    def generate_fakes(self):\n",
        "\n",
        "        ob_space = env.observation_space\n",
        "        reward = 0\n",
        "        success_num = 0\n",
        "\n",
        "\n",
        "        # Por cada episodio\n",
        "        for iteration in range(EPISODES):\n",
        "            # Inicializo todas las variables\n",
        "            observations = []\n",
        "            actions = []\n",
        "            rewards = []\n",
        "            run_policy_steps = 0\n",
        "\n",
        "            truncated=False\n",
        "            terminated=False\n",
        "\n",
        "\n",
        "            #La primera acción de cada episodio se crea con la red neuronal\n",
        "\n",
        "            obs,_=env.reset()\n",
        "\n",
        "            Old_Policy = Policy_net('old_policy', env, obs=obs)\n",
        "\n",
        "            act, v_pred = Old_Policy.act(stochastic=True)\n",
        "\n",
        "            #Convertir de tensor a array\n",
        "            if type(act)=='Tensor':\n",
        "                # Crear una sesión de TensorFlow\n",
        "                sess = tf.compat.v1.Session()\n",
        "\n",
        "                # Evaluar el tensor dentro de la sesión y obtener el resultado como un objeto NumPy ndarray\n",
        "                act = sess.run(act)\n",
        "\n",
        "                # Cerrar la sesión\n",
        "                sess.close()\n",
        "\n",
        "            if isinstance(act, tf.Tensor):\n",
        "                act=act.numpy()\n",
        "\n",
        "            elif isinstance(act, np.ndarray):\n",
        "                act=act\n",
        "\n",
        "\n",
        "            action=int(act)\n",
        "\n",
        "            next_obs,reward,terminated,truncated, info=env.step(action)\n",
        "\n",
        "            # --Actualización de variables: ojo no introduzco el estado y accion inicial, solo introduzco los de PPO\n",
        "            observations.append(next_obs)  # S_0\n",
        "\n",
        "            Policy = Policy_net('policy',env, obs=[next_obs]) # tenemos una política entrenada\n",
        "\n",
        "            # Por cada steps en cada episodio, mientras no se llegue a un estado terminal o un estado malo\n",
        "            while terminated!= True and truncated!= True:\n",
        "                # --Aumentar el numero de steps\n",
        "                run_policy_steps += 1\n",
        "\n",
        "                # --Política para ver la acción asociada al estado\n",
        "                # Las observaciones son un de la forma [[s_0,s_1,s_2,s_3]] por eso su tamaño es (1,4)\n",
        "                action, states_oc = Policy.get_model().predict(next_obs)\n",
        "\n",
        "                action=int(action)\n",
        "\n",
        "                # --Muevo al Agente al siguiente estado\n",
        "                next_obs,reward,terminated,truncated,info=env.step(action)\n",
        "\n",
        "                # --Actualización de variables\n",
        "                actions.append(action) # A_i-1\n",
        "                rewards.append(reward) # R_i-1\n",
        "\n",
        "                # -- Muestro visualización gráfica\n",
        "                # env.render()\n",
        "\n",
        "\n",
        "                # --Si llegamos a un estado final, el juego ha finalizado!!!\n",
        "                # --Se configura el tablero de nuevo\n",
        "                if terminated== True:\n",
        "                    obs = env.reset()\n",
        "                    reward = -1\n",
        "                    break\n",
        "                else:\n",
        "                    observations.append(next_obs) # O_i\n",
        "                    self.obs = next_obs\n",
        "\n",
        "            # Ver si el episodio ha obtendo una recompensa total igual o superior a 195\n",
        "            if sum(rewards) >= 195:\n",
        "                success_num += 1\n",
        "                render = True\n",
        "                if success_num >= 100:\n",
        "                    break\n",
        "            else:\n",
        "                success_num = 0\n",
        "\n",
        "\n",
        "        observations = np.reshape(observations, newshape=[-1] + list(ob_space.shape))\n",
        "        actions = np.array(actions).astype(dtype=np.int32)\n",
        "\n",
        "\n",
        "        return observations, actions, rewards, Old_Policy, Policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09e8d10c"
      },
      "source": [
        "## PPO Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "38bd0d00"
      },
      "outputs": [],
      "source": [
        "# Clase PPOTrain\n",
        "# Tenemos dos politica theta_i y theta_i+1\n",
        "# Almacenamos dos políticas Policy_net(cada una de ella con su PPO) y calculamos el valor gaes a partir de valores\n",
        "# gamma, clip_value, c_1, c_2\n",
        "# Realizamos aqui el entrenamiento, calculo de gradiente y función de pérdida del PPO para despues usarlo en el generador\n",
        "# de la GAN\n",
        "class PPOTrain:\n",
        "\n",
        "    def __init__(self, Policy, Old_Policy, obs, actions, rewards, gamma=0.95, clip_value=0.2, c_1=1, c_2=0.01):\n",
        "        \"\"\"\n",
        "        arg:\n",
        "            Policy\n",
        "            Old_Policy\n",
        "            gamma\n",
        "            clip_value\n",
        "            c_1 parámetro para la diferencia de valores\n",
        "            c_2 parámetro para el bonus de entropía\n",
        "        \"\"\"\n",
        "        self.Policy = Policy\n",
        "        self.Old_Policy = Old_Policy\n",
        "        self.gamma = gamma\n",
        "        self.obs=obs\n",
        "\n",
        "        self.pi_trainable = self.Policy.get_trainable_variables()\n",
        "        self.old_pi_trainable = self.Old_Policy.get_trainable_variables()\n",
        "\n",
        "\n",
        "        policy_name = \"policy\"\n",
        "        old_policy_name=\"policy\"\n",
        "\n",
        "        policy_dict_ = self.pi_trainable[policy_name]\n",
        "        old_policy_dict_=self.old_pi_trainable[old_policy_name]\n",
        "\n",
        "        self.pi=[]\n",
        "        if policy_name in self.pi_trainable:\n",
        "            if old_policy_name in self.old_pi_trainable:\n",
        "                for param_name, param_value in policy_dict_.items():\n",
        "                    # Elimino los pesos que hay en old_policy\n",
        "                    del old_policy_dict_[param_name]\n",
        "                    # Introduzco los pesos de old_policy en policy\n",
        "                    old_policy_dict_[param_name] = param_value\n",
        "                    # OJOO LOS PESOS ESTÁN AQUI\n",
        "                    self.pi.append(param_value)\n",
        "        else:\n",
        "            print(f\"No se encontró la política con el nombre: {policy_name}\")\n",
        "\n",
        "\n",
        "        # Le asignamos old_pi_trainable=pi_trainable ya que ajustaremos unos nuevos pi_trainable\n",
        "\n",
        "\n",
        "        self.actions = actions\n",
        "        self.rewards=rewards\n",
        "        self.v_preds=self.Old_Policy.get_v_preds()\n",
        "        self.v_preds_next=self.Policy.get_v_preds()\n",
        "\n",
        "        #  generative advantage estimator(lambda = 1), see ppo paper eq(11)\n",
        "        self.gaes =self.get_gaes(self.rewards, self.v_preds, self.v_preds_next)\n",
        "\n",
        "        act_probs =self.Policy.get_action_prob()\n",
        "        act_probs_old =self.Old_Policy.get_action_prob()\n",
        "\n",
        "        # probabilities of actions which agent took with policy\n",
        "        act_probs = act_probs * tf.one_hot(indices=self.actions, depth=act_probs.shape[1])\n",
        "        self.act_probs = tf.reduce_sum(act_probs, axis=1)\n",
        "\n",
        "        # probabilities of actions which agent took with old policy\n",
        "        act_probs_old = act_probs_old * tf.one_hot(indices=self.actions, depth=act_probs_old.shape[1])\n",
        "        self.act_probs_old = tf.reduce_sum(act_probs_old, axis=1)\n",
        "\n",
        "        self.loss=loss_fn_ppo(self.act_probs, self.act_probs_old, self.gaes)\n",
        "\n",
        "        self.optimizer =tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    def loss_fn_G(self):\n",
        "        return loss_fn_ppo(self.act_probs, self.act_probs_old, self.gaes)\n",
        "\n",
        "    def get_pi_trainable(self):\n",
        "        return self.pi\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        return self.optimizer\n",
        "\n",
        "    def get_OldPolicy(self):\n",
        "        return self.Old_Policy\n",
        "\n",
        "    def get_Policy(self):\n",
        "        return self.Policy\n",
        "\n",
        "    def get_gaes(self, rewards, v_preds, v_preds_next):\n",
        "        deltas = [r_t + self.gamma * v_next - v for r_t, v_next, v in zip(rewards, v_preds_next, v_preds)]\n",
        "        # calculate generative advantage estimator(lambda = 1), see ppo paper eq(11)\n",
        "        gaes = copy.deepcopy(deltas)\n",
        "        for t in reversed(range(len(gaes) - 1)):  # is T-1, where T is time step which run policy\n",
        "            gaes[t] = gaes[t] + self.gamma * gaes[t + 1]\n",
        "        return gaes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0d6ec49"
      },
      "source": [
        "# GAIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "fcc1c47c"
      },
      "outputs": [],
      "source": [
        "####################################################################################################################\n",
        "# CLASE GAIL\n",
        "####################################################################################################################\n",
        "class GAN(keras.Model):\n",
        "    # Constructor\n",
        "    def __init__(self, discriminator, generator):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator=Policy_net\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    # Compila el modelo GAN inicializando los optimizadores y la función de pérdida del modelo GAN\n",
        "    def compile(self,d_optimizer, loss_fn_D ):\n",
        "        super(GAN, self).compile(run_eagerly=True)\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.loss_fn_D=  loss_fn_D\n",
        "\n",
        "    # Devuelve las métricas obtenidas con el generador y discriminador\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric,self.g_loss_metric]\n",
        "\n",
        "    # Evaluación del Discriminador\n",
        "    def evaluate_D(self, X_test):\n",
        "        len_real = X_test.shape[0]\n",
        "\n",
        "        generate_observations, generate_actions, rewards, Old_Policy, Policy=self.generator.generate_fakes(self.generator)\n",
        "\n",
        "        generate_a_one_hot=np.eye(env.action_space.n)[generate_actions]\n",
        "\n",
        "        dataset_gen=np.concatenate([generate_observations,generate_a_one_hot],axis=1)\n",
        "\n",
        "\n",
        "        len_fakes=dataset_gen.shape[0]\n",
        "\n",
        "        # Compilamos el discriminador como CNN\n",
        "        self.discriminator.discriminator_net.compile(optimizer=self.d_optimizer, loss=self.loss_fn_D, metrics=['accuracy'])\n",
        "\n",
        "        # Evaluamos como CNN\n",
        "        loss_real, acc_real=self.discriminator.discriminator_net.evaluate(X_test[0:len_real], tf.ones((len_real,1)), verbose=1)\n",
        "\n",
        "        loss_fake, acc_fake=self.discriminator.discriminator_net.evaluate(dataset_gen[0:len_fakes],tf.ones((len_fakes,1)), verbose=1)\n",
        "\n",
        "        print('>Loss real: ')\n",
        "        print(loss_real)\n",
        "        print('>Loss fake: ')\n",
        "        print(loss_fake)\n",
        "\n",
        "\n",
        "    # Evaluación del generador\n",
        "    def evaluate_G(self):\n",
        "        # Definimos el entorno\n",
        "        env= gym.make('CartPole-v1')\n",
        "\n",
        "        # Lista donde amacenaremos la recompensa acumulada de cada episodio.\n",
        "        # NUESTRO OBJETIVO: Agente aprenda a tomar las acciones que maximicen la recompensa\n",
        "        rewards=[]\n",
        "\n",
        "        # Para cada episodio, el Agente se mueve por el Entorno mediante acciones hasta llegar a un estado final\n",
        "        # siguiendo la política que se ha aprendido en el entrenamiento de la GAN\n",
        "        for episode in range(EPISODES_EVALUATE_G):\n",
        "            truncated=False\n",
        "            terminated=False\n",
        "            R=0.0\n",
        "            reward=0.0\n",
        "\n",
        "            # Estado inicial del juego\n",
        "            obs,_=env.reset()\n",
        "\n",
        "            #Interactuamos con el Entorno hasta que lleguemos a un estado final\n",
        "            while terminated!= True and truncated!=True:\n",
        "                action, _=self.generator.get_model().predict(obs)\n",
        "                obs,reward,terminated,info=env.step(int(action))\n",
        "\n",
        "                # Incremento la recompensa del episodio i al haber ejecutado el step\n",
        "                R+=reward\n",
        "\n",
        "            rewards.append(R)\n",
        "            # Vemos para el episodio, su recompensa acumulada que es lo que se trata de maximizar\n",
        "            print(\"Episode  {} Total reward: {}\".format(episode,R))\n",
        "\n",
        "        # Cierro el entorno\n",
        "        env.close()\n",
        "\n",
        "        # Muestro las recompensas obtenidas en cada episodio\n",
        "        indices = range(0, EPISODES_EVALUATE_G)\n",
        "        plt.plot(indices,rewards)\n",
        "        plt.show()\n",
        "\n",
        "    def train_step(self, X_train):\n",
        "        # Ojo no tenemos la misma cantidad de datos verdaderos y falsos, por eso calculamos len_real y len_fakes\n",
        "        # No podemos controlar la creación de x secuencias [s,a] ya que generaremos tantas secuencias como se\n",
        "        # necesiten para finalizar el juego\n",
        "\n",
        "        len_real = X_train.shape[0]\n",
        "\n",
        "        batch_size=len_real\n",
        "\n",
        "        generate_observations, generate_actions, rewards, Old_Policy, Policy=self.generator.generate_fakes(self.generator)\n",
        "\n",
        "        generate_a_one_hot=np.eye(env.action_space.n)[generate_actions]\n",
        "\n",
        "        if generate_observations.shape[0] == generate_a_one_hot.shape[0]:\n",
        "          dataset_gen = np.concatenate([generate_observations, generate_a_one_hot], axis=1)\n",
        "        else:\n",
        "          generate_a_one_hot_resized = np.resize(generate_a_one_hot, generate_observations.shape)\n",
        "          dataset_gen = np.concatenate([generate_observations, generate_a_one_hot_resized], axis=1)\n",
        "\n",
        "        len_fakes=dataset_gen.shape[0]\n",
        "\n",
        "        # Las etiquetas de las imagenes combinadas las tenemos que crear nosotros introduciendo algo de ruido\n",
        "        # con tf.random.uniform\n",
        "        labels = tf.concat(\n",
        "          [tf.ones((len_real, 1)), tf.zeros((len_fakes, 1))],\n",
        "          axis=0\n",
        "        )\n",
        "\n",
        "\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        if X_train.shape[1] == dataset_gen.shape[1]:\n",
        "            # Las dimensiones coinciden, se puede realizar la concatenación\n",
        "          combined_images = tf.concat([X_train, dataset_gen], axis=0)\n",
        "        else:\n",
        "          # Aquí debes realizar las modificaciones necesarias para asegurarte de que las dimensiones sean compatibles\n",
        "\n",
        "          # Por ejemplo, si quieres asegurarte de que ambas dimensiones 1 tengan el mismo tamaño,\n",
        "          # puedes ajustar el tamaño de dataset_gen para que coincida con el tamaño de X_train\n",
        "          dataset_gen_resized = dataset_gen[:, :X_train.shape[1]]  # Ajustar el tamaño al tamaño de X_train\n",
        "\n",
        "          # Luego, realizar la concatenación\n",
        "          combined_images = tf.concat([X_train, dataset_gen_resized], axis=0)\n",
        "\n",
        "        #############  PASO 1:  ENTRENAMIENTO DEL DISCRIMINADOR ##############################################\n",
        "\n",
        "        # Entrenamiento del discriminador con las [s,a] del agente y del experto combinadas, esto es,\n",
        "        # le pasamos un conjunto que tiene tanto imágenes reales como imágenes falsas\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions=np.zeros((len_real+len_fakes,6))\n",
        "            # Predicciones obtenidas con el discriminador\n",
        "            predictions = self.discriminator.discriminator_net(combined_images)\n",
        "            # Valor de la función de pérdida al comparar las predicciones con las etiquetas reales\n",
        "            d_loss = self.loss_fn_D(labels, predictions)\n",
        "\n",
        "        # Calculo del gradiente y actualización del gradiente\n",
        "        grads = tape.gradient(d_loss, self.discriminator.getNet().trainable_weights)\n",
        "\n",
        "        self.d_optimizer.apply_gradients(\n",
        "          zip(grads, self.discriminator.getNet().trainable_weights)\n",
        "        )\n",
        "\n",
        "\n",
        "        d_rewards = discriminator.getRewards()\n",
        "\n",
        "\n",
        "        ############# PASO 2: ENTRENAMIENTO DEL GENERADOR=POLÍTICA  ##############################\n",
        "\n",
        "        ppotrain=PPOTrain(Policy,Old_Policy,actions=generate_actions,rewards=rewards, obs=generate_observations[0])\n",
        "\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            g_loss = ppotrain.loss_fn_G()\n",
        "\n",
        "\n",
        "        g_loss = tf.cast(g_loss, dtype=tf.float32)\n",
        "\n",
        "\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "\n",
        "        return {\"d_loss\": self.d_loss_metric.result(),\n",
        "                    \"g_loss\": self.g_loss_metric.result()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfa28473"
      },
      "source": [
        "# Lectura de base de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCarNjDkXTeb"
      },
      "source": [
        "## Base de datos experta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21763a85",
        "outputId": "5e3ae02b-c4d1-4b69-d45f-1181b22e2f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\tEstados reales: \n",
            " [[-0.04456399  0.04653909  0.01326909 -0.02099827]\n",
            " [-0.04363321  0.24146827  0.01284913 -0.3094653 ]\n",
            " [-0.03880385  0.04616562  0.00665982 -0.01275795]\n",
            " ...\n",
            " [-0.69562376 -0.8861578  -0.07125074  0.6065587 ]\n",
            " [-0.71334696 -0.69011575 -0.05911956  0.29231167]\n",
            " [-0.72714925 -0.8843471  -0.05327333  0.56577873]]\n",
            "\tAcciones reales: \n",
            " [1 0 0 ... 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "expert_observations = np.genfromtxt('/content/drive/MyDrive/Colab Notebooks/observations.csv')\n",
        "expert_actions = np.genfromtxt('/content/drive/MyDrive/Colab Notebooks/actions.csv', dtype=np.int32)\n",
        "\n",
        "print(\"\\n\\tEstados reales: \\n\", expert_observations)\n",
        "print(\"\\tAcciones reales: \\n\", expert_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49ac2e89",
        "outputId": "27b4e712-1bd8-4db7-f5ab-1d32d5e0bd58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.04456399  0.04653909  0.01326909 -0.02099827  0.          1.        ]\n",
            " [-0.04363321  0.24146827  0.01284913 -0.3094653   1.          0.        ]\n",
            " [-0.03880385  0.04616562  0.00665982 -0.01275795  1.          0.        ]\n",
            " ...\n",
            " [-0.69562376 -0.8861578  -0.07125074  0.6065587   0.          1.        ]\n",
            " [-0.71334696 -0.69011575 -0.05911956  0.29231167  1.          0.        ]\n",
            " [-0.72714925 -0.8843471  -0.05327333  0.56577873  0.          1.        ]]\n",
            "Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento: 3200\n",
            "Nº de (ESTADOS,ACCIONES) en el conjunto de prueba: 800\n"
          ]
        }
      ],
      "source": [
        "# Construimos el dataset [s,a] reales y lo dividimos en training y test\n",
        "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
        "\n",
        "dataset=np.concatenate([expert_observations,expert_a_one_hot],axis=1)\n",
        "\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "# Divide los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
        "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
        "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa3ae613"
      },
      "source": [
        "# Definición de generador, dicriminador y generamos [s,a]^*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "92e4545a"
      },
      "outputs": [],
      "source": [
        "env= gym.make('CartPole-v1')\n",
        "obs,_= env.reset()\n",
        "\n",
        "# Generador\n",
        "generator=Policy_net( 'policy', env, obs)\n",
        "\n",
        "# Generamos [s,a] falsas y las políticas theta_i y theta_i+1\n",
        "observations, actions, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
        "\n",
        "# Discriminador\n",
        "discriminator=Discriminator(env, discriminator_net, expert_observations, expert_actions, observations, actions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50f9e910"
      },
      "source": [
        "# EXPERIMENTACIÓN DE GAIL CON CARTPOLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWoOtQApX1sO"
      },
      "source": [
        "## Definición de GAIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "urrCBIpLHMKo"
      },
      "outputs": [],
      "source": [
        "gan=GAN(discriminator=discriminator,generator=generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj6tieO1X41j"
      },
      "source": [
        "## Compilación de GAIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "feb5b0d5"
      },
      "outputs": [],
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "gan.compile(\n",
        "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss_fn_D=loss_fn_D\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1q2qSu_YBDc"
      },
      "source": [
        "## Entrenamiento de GAIL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d917ecb9"
      },
      "source": [
        "En la ejecución de la primera época se puede ver como g_loss empieza a disminuir mientras d_loss empieza a aumentar,\n",
        "un comportamiento normal en las GANs ya que el generador produce muestras con intención de engañar al discriminador y lo consigue.\n",
        "\n",
        "Cuando el Discriminador aprenda los patrones con los que el generador esta generando las secuencias falsas, entonces\n",
        "el discriminador habra aprendido y su pérdida disminuirá mientras que g_loss incrementa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0e55429",
        "outputId": "7f8bc816-8c37-44a7-e233-fdd34f5a6b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 85s 85s/step - d_loss: 0.8025 - g_loss: -1.1011\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.7921 - g_loss: -1.0477\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.7949 - g_loss: -1.0084\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.7866 - g_loss: -1.0694\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.7554 - g_loss: -1.0597\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.7397 - g_loss: -0.8605\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.7374 - g_loss: -0.7654\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 83s 83s/step - d_loss: 0.7252 - g_loss: -1.0063\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 83s 83s/step - d_loss: 0.7136 - g_loss: nan\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.7103 - g_loss: -0.8803\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.7806 - g_loss: -1.0473\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6908 - g_loss: nan\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 82s 82s/step - d_loss: 0.7045 - g_loss: -1.0506\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 82s 82s/step - d_loss: 0.6907 - g_loss: -0.9901\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6930 - g_loss: -1.0224\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6989 - g_loss: -0.8705\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 85s 85s/step - d_loss: 0.6892 - g_loss: -0.8569\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6515 - g_loss: -1.2948\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.7553 - g_loss: -1.1600\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 85s 85s/step - d_loss: 0.6345 - g_loss: -1.0346\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6410 - g_loss: -1.1648\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6264 - g_loss: -1.0410\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6633 - g_loss: -0.5686\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6096 - g_loss: -0.9304\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6500 - g_loss: -0.8605\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6025 - g_loss: -1.1692\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.6007 - g_loss: -0.8887\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 83s 83s/step - d_loss: 0.6294 - g_loss: -0.8368\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 83s 83s/step - d_loss: 0.5750 - g_loss: -1.0466\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.5776 - g_loss: -0.9062\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.5883 - g_loss: -1.1347\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 84s 84s/step - d_loss: 0.5915 - g_loss: -0.6838\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 83s 83s/step - d_loss: 0.5464 - g_loss: -1.1514\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 83s 83s/step - d_loss: 0.5383 - g_loss: nan\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 88s 88s/step - d_loss: 0.5490 - g_loss: -1.0593\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 89s 89s/step - d_loss: 0.5383 - g_loss: -0.8167\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 88s 88s/step - d_loss: 0.5444 - g_loss: -0.7831\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 87s 87s/step - d_loss: 0.5364 - g_loss: -0.5313\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 87s 87s/step - d_loss: 0.5108 - g_loss: -1.0652\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 86s 86s/step - d_loss: 0.4975 - g_loss: -0.7597\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 86s 86s/step - d_loss: 0.5835 - g_loss: -0.9496\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 85s 85s/step - d_loss: 0.5068 - g_loss: -0.9312\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 85s 85s/step - d_loss: 0.4930 - g_loss: -0.9543\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 85s 85s/step - d_loss: 0.4872 - g_loss: -0.7284\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 85s 85s/step - d_loss: 0.4759 - g_loss: -1.0807\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 85s 85s/step - d_loss: 0.4661 - g_loss: -0.6479\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 86s 86s/step - d_loss: 0.5113 - g_loss: -1.0819\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 86s 86s/step - d_loss: 0.4351 - g_loss: -0.9976\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 86s 86s/step - d_loss: 0.4467 - g_loss: -0.9339\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 85s 85s/step - d_loss: 0.5227 - g_loss: -0.9684\n"
          ]
        }
      ],
      "source": [
        "# Deshabilitar los mensajes de información de TensorFlow\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# Deshabilitar los mensajes de información de OpenAI Gym\n",
        "gym.logger.set_level(40)\n",
        "\n",
        "\n",
        "history=gan.fit(X_train,\n",
        "    epochs=EPOCHS, batch_size=3200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0K3-f_cYFX0"
      },
      "source": [
        "## Evaluación del Discriminador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "a6d7c1b7",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1489927-2d15-4935-eeed-f5c13114d42e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 0s 12ms/step - loss: 0.4041 - accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3958 - accuracy: 0.0000e+00\n",
            ">Loss real: \n",
            "0.4040694832801819\n",
            ">Loss fake: \n",
            "0.39577269554138184\n"
          ]
        }
      ],
      "source": [
        "# Evaluamos el Discriminador de GAIL en el TEST\n",
        "gan.evaluate_D(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdah2vFDYTMR"
      },
      "source": [
        "## Evaluación del Generador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "5ac18f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "outputId": "79a1479d-fec2-45e0-a007-9bbd3aba3498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode  0 Total reward: 44.0\n",
            "Episode  1 Total reward: 10.0\n",
            "Episode  2 Total reward: 55.0\n",
            "Episode  3 Total reward: 23.0\n",
            "Episode  4 Total reward: 55.0\n",
            "Episode  5 Total reward: 46.0\n",
            "Episode  6 Total reward: 29.0\n",
            "Episode  7 Total reward: 40.0\n",
            "Episode  8 Total reward: 22.0\n",
            "Episode  9 Total reward: 14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Evaluamos el modelo GENERADOR en el TEST\n",
        "\n",
        "# Definimos el entorno\n",
        "env= gym.make('CartPole-v1')\n",
        "\n",
        "\n",
        "rewards=[]\n",
        "# Para cada episodio, el Agente se mueve por el Entorno mediante acciones hasta llegar a un estado final\n",
        "# siguiendo la política que se ha aprendido en el entrenamiento de la GAN\n",
        "for episode in range(10):\n",
        "    truncated=False\n",
        "    terminated=False\n",
        "    R=0.0\n",
        "    reward=0.0\n",
        "\n",
        "    # Estado inicial del juego\n",
        "    obs,_=env.reset()\n",
        "\n",
        "    #Interactuamos con el Entorno hasta que lleguemos a un estado final\n",
        "    while terminated!= True and truncated!=True:\n",
        "        action, _=generator.get_model().predict(obs)\n",
        "        obs,reward,terminated,truncated,info=env.step(int(action))\n",
        "\n",
        "        # Incremento la recompensa del episodio i al haber ejecutado el step\n",
        "        R+=reward\n",
        "\n",
        "    rewards.append(R)\n",
        "\n",
        "    # Vemos para el episodio, su recompensa acumulada que es lo que se trata de maximizar\n",
        "    print(\"Episode  {} Total reward: {}\".format(episode,R))\n",
        "\n",
        "# Cierro el entorno\n",
        "env.close()\n",
        "\n",
        "# Muestro las recompensas obtenidas en cada episodio\n",
        "indices = range(0, 10)\n",
        "plt.plot(indices,rewards)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}