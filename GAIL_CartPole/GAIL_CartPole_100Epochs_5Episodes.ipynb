{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4786295",
    "outputId": "f1ca4aff-a7aa-462c-e327-95a2d48f6829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 13 08:10:41 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   77C    P0    32W /  70W |   1119MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "## COMPROBAR GPU ASIGNADA EN COLABORATORY\n",
    "#########################################################################\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1QxgGVZ_PvV",
    "outputId": "ce82155d-22ca-40de-befa-c175c46b269c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "## MONTAR DRIVE\n",
    "#########################################################################\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZZ0uPW9MxU2",
    "outputId": "b7dcfb87-214f-4cfa-d715-bc460e7bd7bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
      "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.22.4)\n",
      "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.0.1+cu118)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable_baselines3) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable_baselines3) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable_baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (3.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable_baselines3) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable_baselines3) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11->stable_baselines3) (16.0.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->stable_baselines3) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable_baselines3) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "a6cef0d7"
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "## LIBRERIAS NECESARIAS\n",
    "#########################################################################\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "import copy\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "abc775a7"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Variables globales\n",
    "###########################################################################\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=100\n",
    "EPISODES=5\n",
    "EPISODES_EVALUATE_G=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85cff0c4"
   },
   "source": [
    "# Gym CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39e0a1c1"
   },
   "source": [
    "Un péndulo está unido mediante una articulación no accionada a un carro que se desplaza a lo largo de una pista sin fricción. El péndulo se coloca verticalmente sobre el carro y el objetivo es equilibrar el poste aplicando fuerzas en dirección izquierda y derecha sobre el carro.\n",
    "\n",
    "**Espacio de Acciones**: Espacio discreto de tamaño (2)\n",
    " * Acción 0: Empujar el carro hacia la izquierda\n",
    " * Acción 1: Empujar el carro hacia la derecha\n",
    "\n",
    "    \n",
    "   \n",
    "**Espacio de Observaciones**: Espacio discreto de tamaño (4,)\n",
    "\n",
    "* La observación es un ndarray con forma (4,) con los valores correspondientes a las siguientes posiciones y velocidades:\n",
    "         Num    Observación                         Min                            Max\n",
    "\n",
    "          0    Posición del Carro                 - 4.8                            4.8\n",
    "\n",
    "          1    Velocidad del Carro                 -Inf                            Inf\n",
    "\n",
    "          2    Ángulo del Poste              ~ -0.418 rad (-24°)          ~ 0.418 rad (24°)\n",
    "\n",
    "          3    Velocidad Angular del Poste         -Inf                            Inf\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82e0467a",
    "outputId": "b667a4f3-9381-4435-a008-cdd42c3ddb12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Definimos el entorno\n",
    "env= gym.make('CartPole-v1')\n",
    "\n",
    "# Obtenemos el espacio de estados y acciones del entorno\n",
    "ob_space=env.observation_space\n",
    "\n",
    "# Mostramos el número de acciones del entorno\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9caa49f2"
   },
   "source": [
    "\n",
    "\n",
    "# Discriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kshaCKl6UIJb"
   },
   "source": [
    "## Red neuronal del Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d9e6597",
    "outputId": "d5a49b40-c98b-4587-8774-8d7980a7aa4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, None, 20)          140       \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, None, 20)          420       \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, None, 20)          420       \n",
      "                                                                 \n",
      " prob (Dense)                (None, None, 1)           21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,001\n",
      "Trainable params: 1,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Red neuronal del Discriminador\n",
    "# Input: listas de longitud 6, [s,a]\n",
    "# Output: probabilidad de real o falso [0,1]\n",
    "discriminator_net=keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(None,ob_space.shape[0]+2)),\n",
    "        layers.Dense(units=20,activation=tf.nn.leaky_relu, name='layer1'),\n",
    "        layers.Dense(units=20,activation=tf.nn.leaky_relu, name='layer2'),\n",
    "        layers.Dense(units=20, activation=tf.nn.leaky_relu, name='layer3'),\n",
    "        layers.Dense(units=1, activation=tf.sigmoid, name='prob'),\n",
    "\n",
    "    ],\n",
    "    name=\"discriminator_net\"\n",
    "\n",
    ")\n",
    "discriminator_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMIc-9PoUNdG"
   },
   "source": [
    "## Función de pérdida del Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "78bdb460"
   },
   "outputs": [],
   "source": [
    "# Función de pérdida del Discriminador\n",
    "# prob1=>output de la red neuronal anterior cuando recibe como entrada [s,a] de la base de datos, REAL\n",
    "# prob2=>output de la red neuronal anterior cuando recibe como entrada [s,a] FALSO\n",
    "def loss_fn_D(prob1, prob2):\n",
    "    # Esperanza del logaritmo de la D(x)=salida de la red neuronal cuando x=entrada REA\n",
    "    loss_expert=tf.reduce_mean(tf.math.log(tf.clip_by_value(prob1,0.01,1)))\n",
    "\n",
    "    # Esperanza del logaritmo de 1-D(x) donde D(x)=salida de la red neuronal cuando x=entrada FALSA\n",
    "    loss_agent=tf.reduce_mean(tf.math.log(tf.clip_by_value(1-prob2,0.01,1)))\n",
    "    loss_expert=tf.cast(loss_expert, dtype=tf.float32)\n",
    "    loss_agent=tf.cast(loss_agent, dtype=tf.float32)\n",
    "    loss=loss_expert+loss_agent\n",
    "    loss=-loss\n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_R6wBgyUSYo"
   },
   "source": [
    "## Clase del Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5e19555d"
   },
   "outputs": [],
   "source": [
    "# Clase DISCRIMINADOR\n",
    "class Discriminator:\n",
    "    def __init__(self, env, discriminator_net, expert_s, expert_a, agent_s, agent_a):\n",
    "        # -Red neuronal del Discriminador\n",
    "        self.discriminator_net=discriminator_net\n",
    "        # -Experto: [s,a]\n",
    "        self.expert_s=expert_s\n",
    "        self.expert_a=expert_a\n",
    "        expert_a_one_hot=tf.one_hot(self.expert_a,depth=env.action_space.n)\n",
    "        # Añadimos ruido para estabilizar el entrenamiento\n",
    "        expert_a_one_hot+= tf.random.normal(tf.shape(expert_a_one_hot), mean=0.2, stddev=0.1, dtype=tf.float32)/1.2\n",
    "        self.expert_s_a=tf.concat([self.expert_s,expert_a_one_hot],axis=1)\n",
    "\n",
    "        # -Agente:  [s,a]\n",
    "        self.agent_s=agent_s\n",
    "        self.agent_a=agent_a\n",
    "        agent_a_one_hot=tf.one_hot(self.agent_a,depth=env.action_space.n)\n",
    "        agent_a_one_hot+= tf.random.normal(tf.shape(agent_a_one_hot), mean=0.2, stddev=0.1, dtype=tf.float32)/1.2\n",
    "        self.agent_s_a=tf.concat([self.agent_s,agent_a_one_hot],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        # Calculamos la salida de la red para [s,a] del experto y del agente ya que lo necesitamos para reward\n",
    "\n",
    "        # -Salida de la red neuronal Discriminador para [s,a] expertos(verdaderos)\n",
    "        self.prob_expert=self.discriminator_net(self.expert_s_a)\n",
    "\n",
    "        # -Salida  de la red neuronal Discrimiinador para [s,a] Agente(falsos)\n",
    "        self.prob_agent=self.discriminator_net(self.agent_s_a)\n",
    "\n",
    "        #-Recompensa obtenida cuando el Agente realiza [s,a] falsas\n",
    "        self.rewards=tf.math.log(tf.clip_by_value(self.prob_agent,1e-10,1)) #log(P(expert|s,a)) cuando mas grande es mejor el agente\n",
    "\n",
    "\n",
    "    def getNet(self):\n",
    "        return self.discriminator_net\n",
    "\n",
    "    def getAgent_S_A(self):\n",
    "        return self.agent_s_a\n",
    "\n",
    "    def getExpert_S_A(self):\n",
    "        return self.expert_s_a\n",
    "\n",
    "    def getProb(self):\n",
    "        return self.prob_expert, self.prob_agent\n",
    "\n",
    "    def getRewards(self):\n",
    "        return self.rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f540f7a"
   },
   "source": [
    "# Generador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQRVvHKWUWBU"
   },
   "source": [
    "## Redes neuronales del Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce26c866",
    "outputId": "f8b0a39d-02bd-44bc-9bf2-86a12669c921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_net_Act\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, None, 20)          100       \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, None, 20)          420       \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, None, 2)           42        \n",
      "                                                                 \n",
      " layer4 (Dense)              (None, None, 2)           6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 568\n",
      "Trainable params: 568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Red neuronal del Generador donde se producen acciones\n",
    "# Input: estados, listas de tamaño 4, s=[s0,s1,s2,s3]\n",
    "# Output: acciones, listas de tamaño 2, a=[a0,a1]\n",
    "generator_net_Act=keras.Sequential(\n",
    "    [\n",
    "            keras.Input(shape=(None,ob_space.shape[0])),\n",
    "            layers.Dense(units=20, activation=tf.tanh,name='layer1'),\n",
    "            layers.Dense(units=20, activation=tf.tanh, name='layer2'),\n",
    "            layers.Dense(units=2, activation=tf.tanh, name='layer3'),\n",
    "            layers.Dense(units=2, activation=tf.nn.softmax, name='layer4')\n",
    "\n",
    "        ],\n",
    "    name=\"generator_net_Act\"\n",
    ")\n",
    "\n",
    "generator_net_Act.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "429f8080",
    "outputId": "fbb7fe04-7fa4-4f7d-8e0a-ab48fcadca32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_v_preds\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, None, 20)          100       \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, None, 20)          420       \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, None, 1)           21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 541\n",
      "Trainable params: 541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Red neuronal del Generador donde se producen v_pred\n",
    "# Input: estados, listas de tamaño 4, s=[s0,s1,s2,s3]\n",
    "# Output: v_pred, listas de tamaño 1, v_pred\n",
    "\n",
    "generator_net_v_preds=keras.Sequential(\n",
    "    [\n",
    "            keras.Input(shape=(None,ob_space.shape[0])),\n",
    "            layers.Dense(units=20, activation=tf.tanh,name='layer1'),\n",
    "            layers.Dense(units=20, activation=tf.tanh, name='layer2'),\n",
    "            layers.Dense(units=1, activation=None, name='layer3'),\n",
    "        ],\n",
    "    name=\"generator_v_preds\"\n",
    ")\n",
    "\n",
    "generator_net_v_preds.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOU1XlPtUeOG"
   },
   "source": [
    "## Función de pérdida del Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "0753302f"
   },
   "outputs": [],
   "source": [
    "# Función de pérdida del Generador\n",
    "def loss_fn_ppo(act_probs, act_probs_old, gaes, clip_value=0.2):\n",
    "    #--> Construir el calculo del grafo para loss_clip\n",
    "\n",
    "    ratios=tf.exp(tf.math.log(tf.clip_by_value(act_probs, 1e-10, 1.0))\n",
    "                    - tf.math.log(tf.clip_by_value(act_probs_old, 1e-10, 1.0)))\n",
    "\n",
    "    clipped_ratios= tf.clip_by_value(ratios, clip_value_min=1 - clip_value, clip_value_max=1 + clip_value)\n",
    "    loss_clip=tf.minimum(tf.multiply(gaes, ratios), tf.multiply(gaes, clipped_ratios))\n",
    "    loss_clip = tf.reduce_mean(loss_clip)\n",
    "\n",
    "    # minimizar -loss == maximizar loss\n",
    "    loss = -loss_clip\n",
    "    tf.summary.scalar('total', loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Orx8ZvtGUirZ"
   },
   "source": [
    "## Clase del Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ad26b1b9"
   },
   "outputs": [],
   "source": [
    "# Clase del GENERADOR: política con su optimizador PPO\n",
    "# Observesé que cada generador implementa una política distinta, por tanto, se ha decidido llamar a la clase\n",
    "# Policy_net en lugar de generator\n",
    "class Policy_net:\n",
    "    def __init__(self, name: str, env, obs):\n",
    "        \"\"\"\n",
    "        env: gym env\n",
    "        obs:\n",
    "        \"\"\"\n",
    "        # -Entorno\n",
    "        self.env=env\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "        # -El algoritmo de Optimización de Política Proximal, PPO, combina ideas del algoritmo A2C\n",
    "        # (que utiliza múltiples trabajadores) y del algoritmo TRPO (que utiliza una región de confianza para\n",
    "        # mejorar el actor).\n",
    "        self.model=PPO(policy=\"MlpPolicy\", env=env, verbose=0)\n",
    "\n",
    "\n",
    "        self.model.learn(total_timesteps=25)\n",
    "\n",
    "        # -Observación inicial a partir de la cual se crean las acciones iniciales\n",
    "        # haciendo uso de las redes neuronales del generador\n",
    "        self.obs=np.reshape(np.array(obs),(1,4))\n",
    "\n",
    "        # Utilizamos las dos redes neuronales que hemos creado : generator_net_Act y generator_net_v_preds\n",
    "        # V_pred=>recompensa media de que un agente ejecute una acción\n",
    "\n",
    "        # -Acción inicial generada con red neuronal y v_pred con red neuronal\n",
    "        self.act_probs =generator_net_Act(self.obs)\n",
    "        self.v_preds = generator_net_v_preds(self.obs)\n",
    "\n",
    "        # -Accion estocástica inicial\n",
    "        self.act_stochastic = tf.random.categorical(tf.math.log(self.act_probs), num_samples=1)\n",
    "\n",
    "        # -Acción determinística inicial\n",
    "        self.act_deterministic = tf.argmax(self.act_probs, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Para cada estado obs me dice la acción que el agente va a ejecutar sobre el entorno junto con v_pred\n",
    "    # La elección de la acción puede ser estocástica o determinística\n",
    "    def act(self, stochastic=True):\n",
    "        if stochastic:\n",
    "            return self.act_stochastic, self.v_preds\n",
    "        else:\n",
    "            return self.act_deterministic, self.v_preds\n",
    "\n",
    "    def get_action_prob(self):\n",
    "        return self.act_probs\n",
    "\n",
    "    def get_v_preds(self):\n",
    "        return self.v_preds\n",
    "\n",
    "    def get_obs(self):\n",
    "        return self.obs\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_trainable_variables(self):\n",
    "        return self.model.get_parameters()\n",
    "\n",
    "    # Generar [s,a] falsos\n",
    "    def generate_fakes(self):\n",
    "\n",
    "        ob_space = env.observation_space\n",
    "        reward = 0\n",
    "        success_num = 0\n",
    "\n",
    "\n",
    "        # Por cada episodio\n",
    "        for iteration in range(EPISODES):\n",
    "            # Inicializo todas las variables\n",
    "            observations = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            run_policy_steps = 0\n",
    "\n",
    "            truncated=False\n",
    "            terminated=False\n",
    "\n",
    "\n",
    "            #La primera acción de cada episodio se crea con la red neuronal\n",
    "\n",
    "            obs,_=env.reset()\n",
    "\n",
    "            Old_Policy = Policy_net('old_policy', env, obs=obs)\n",
    "\n",
    "            act, v_pred = Old_Policy.act(stochastic=True)\n",
    "\n",
    "            #Convertir de tensor a array\n",
    "            if type(act)=='Tensor':\n",
    "                # Crear una sesión de TensorFlow\n",
    "                sess = tf.compat.v1.Session()\n",
    "\n",
    "                # Evaluar el tensor dentro de la sesión y obtener el resultado como un objeto NumPy ndarray\n",
    "                act = sess.run(act)\n",
    "\n",
    "                # Cerrar la sesión\n",
    "                sess.close()\n",
    "\n",
    "            if isinstance(act, tf.Tensor):\n",
    "                act=act.numpy()\n",
    "\n",
    "            elif isinstance(act, np.ndarray):\n",
    "                act=act\n",
    "\n",
    "\n",
    "            action=int(act)\n",
    "\n",
    "            next_obs,reward,terminated,truncated, info=env.step(action)\n",
    "\n",
    "            # --Actualización de variables: ojo no introduzco el estado y accion inicial, solo introduzco los de PPO\n",
    "            observations.append(next_obs)  # S_0\n",
    "\n",
    "            Policy = Policy_net('policy',env, obs=[next_obs]) # tenemos una política entrenada\n",
    "\n",
    "            # Por cada steps en cada episodio, mientras no se llegue a un estado terminal o un estado malo\n",
    "            while terminated!= True and truncated!= True:\n",
    "                # --Aumentar el numero de steps\n",
    "                run_policy_steps += 1\n",
    "\n",
    "                # --Política para ver la acción asociada al estado\n",
    "                # Las observaciones son un de la forma [[s_0,s_1,s_2,s_3]] por eso su tamaño es (1,4)\n",
    "                action, states_oc = Policy.get_model().predict(next_obs)\n",
    "\n",
    "                action=int(action)\n",
    "\n",
    "                # --Muevo al Agente al siguiente estado\n",
    "                next_obs,reward,terminated,truncated,info=env.step(action)\n",
    "\n",
    "                # --Actualización de variables\n",
    "                actions.append(action) # A_i-1\n",
    "                rewards.append(reward) # R_i-1\n",
    "\n",
    "                # -- Muestro visualización gráfica\n",
    "                # env.render()\n",
    "\n",
    "\n",
    "                # --Si llegamos a un estado final, el juego ha finalizado!!!\n",
    "                # --Se configura el tablero de nuevo\n",
    "                if terminated== True:\n",
    "                    obs = env.reset()\n",
    "                    reward = -1\n",
    "                    break\n",
    "                else:\n",
    "                    observations.append(next_obs) # O_i\n",
    "                    self.obs = next_obs\n",
    "\n",
    "            # Ver si el episodio ha obtendo una recompensa total igual o superior a 195\n",
    "            if sum(rewards) >= 195:\n",
    "                success_num += 1\n",
    "                render = True\n",
    "                if success_num >= 100:\n",
    "                    break\n",
    "            else:\n",
    "                success_num = 0\n",
    "\n",
    "\n",
    "        observations = np.reshape(observations, newshape=[-1] + list(ob_space.shape))\n",
    "        actions = np.array(actions).astype(dtype=np.int32)\n",
    "\n",
    "\n",
    "        return observations, actions, rewards, Old_Policy, Policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09e8d10c"
   },
   "source": [
    "## PPO Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "38bd0d00"
   },
   "outputs": [],
   "source": [
    "# Clase PPOTrain\n",
    "# Tenemos dos politica theta_i y theta_i+1\n",
    "# Almacenamos dos políticas Policy_net(cada una de ella con su PPO) y calculamos el valor gaes a partir de valores\n",
    "# gamma, clip_value, c_1, c_2\n",
    "# Realizamos aqui el entrenamiento, calculo de gradiente y función de pérdida del PPO para despues usarlo en el generador\n",
    "# de la GAN\n",
    "class PPOTrain:\n",
    "\n",
    "    def __init__(self, Policy, Old_Policy, obs, actions, rewards, gamma=0.95, clip_value=0.2, c_1=1, c_2=0.01):\n",
    "        \"\"\"\n",
    "        arg:\n",
    "            Policy\n",
    "            Old_Policy\n",
    "            gamma\n",
    "            clip_value\n",
    "            c_1 parámetro para la diferencia de valores\n",
    "            c_2 parámetro para el bonus de entropía\n",
    "        \"\"\"\n",
    "        self.Policy = Policy\n",
    "        self.Old_Policy = Old_Policy\n",
    "        self.gamma = gamma\n",
    "        self.obs=obs\n",
    "\n",
    "        self.pi_trainable = self.Policy.get_trainable_variables()\n",
    "        self.old_pi_trainable = self.Old_Policy.get_trainable_variables()\n",
    "\n",
    "\n",
    "        policy_name = \"policy\"\n",
    "        old_policy_name=\"policy\"\n",
    "\n",
    "        policy_dict_ = self.pi_trainable[policy_name]\n",
    "        old_policy_dict_=self.old_pi_trainable[old_policy_name]\n",
    "\n",
    "        self.pi=[]\n",
    "        if policy_name in self.pi_trainable:\n",
    "            if old_policy_name in self.old_pi_trainable:\n",
    "                for param_name, param_value in policy_dict_.items():\n",
    "                    # Elimino los pesos que hay en old_policy\n",
    "                    del old_policy_dict_[param_name]\n",
    "                    # Introduzco los pesos de old_policy en policy\n",
    "                    old_policy_dict_[param_name] = param_value\n",
    "                    # OJOO LOS PESOS ESTÁN AQUI\n",
    "                    self.pi.append(param_value)\n",
    "        else:\n",
    "            print(f\"No se encontró la política con el nombre: {policy_name}\")\n",
    "\n",
    "\n",
    "        # Le asignamos old_pi_trainable=pi_trainable ya que ajustaremos unos nuevos pi_trainable\n",
    "\n",
    "\n",
    "        self.actions = actions\n",
    "        self.rewards=rewards\n",
    "        self.v_preds=self.Old_Policy.get_v_preds()\n",
    "        self.v_preds_next=self.Policy.get_v_preds()\n",
    "\n",
    "        #  generative advantage estimator(lambda = 1), see ppo paper eq(11)\n",
    "        self.gaes =self.get_gaes(self.rewards, self.v_preds, self.v_preds_next)\n",
    "\n",
    "        act_probs =self.Policy.get_action_prob()\n",
    "        act_probs_old =self.Old_Policy.get_action_prob()\n",
    "\n",
    "        # probabilities of actions which agent took with policy\n",
    "        act_probs = act_probs * tf.one_hot(indices=self.actions, depth=act_probs.shape[1])\n",
    "        self.act_probs = tf.reduce_sum(act_probs, axis=1)\n",
    "\n",
    "        # probabilities of actions which agent took with old policy\n",
    "        act_probs_old = act_probs_old * tf.one_hot(indices=self.actions, depth=act_probs_old.shape[1])\n",
    "        self.act_probs_old = tf.reduce_sum(act_probs_old, axis=1)\n",
    "\n",
    "        self.loss=loss_fn_ppo(self.act_probs, self.act_probs_old, self.gaes)\n",
    "\n",
    "        self.optimizer =tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    def loss_fn_G(self):\n",
    "        return loss_fn_ppo(self.act_probs, self.act_probs_old, self.gaes)\n",
    "\n",
    "    def get_pi_trainable(self):\n",
    "        return self.pi\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_OldPolicy(self):\n",
    "        return self.Old_Policy\n",
    "\n",
    "    def get_Policy(self):\n",
    "        return self.Policy\n",
    "\n",
    "    def get_gaes(self, rewards, v_preds, v_preds_next):\n",
    "        deltas = [r_t + self.gamma * v_next - v for r_t, v_next, v in zip(rewards, v_preds_next, v_preds)]\n",
    "        # calculate generative advantage estimator(lambda = 1), see ppo paper eq(11)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(gaes) - 1)):  # is T-1, where T is time step which run policy\n",
    "            gaes[t] = gaes[t] + self.gamma * gaes[t + 1]\n",
    "        return gaes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0d6ec49"
   },
   "source": [
    "# GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "fcc1c47c"
   },
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# CLASE GAIL\n",
    "####################################################################################################################\n",
    "class GAN(keras.Model):\n",
    "    # Constructor\n",
    "    def __init__(self, discriminator, generator):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator=Policy_net\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    # Compila el modelo GAN inicializando los optimizadores y la función de pérdida del modelo GAN\n",
    "    def compile(self,d_optimizer, loss_fn_D ):\n",
    "        super(GAN, self).compile(run_eagerly=True)\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_fn_D=  loss_fn_D\n",
    "\n",
    "    # Devuelve las métricas obtenidas con el generador y discriminador\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric,self.g_loss_metric]\n",
    "\n",
    "    # Evaluación del Discriminador\n",
    "    def evaluate_D(self, X_test):\n",
    "        len_real = X_test.shape[0]\n",
    "\n",
    "        generate_observations, generate_actions, rewards, Old_Policy, Policy=self.generator.generate_fakes(self.generator)\n",
    "\n",
    "        generate_a_one_hot=np.eye(env.action_space.n)[generate_actions]\n",
    "\n",
    "        dataset_gen=np.concatenate([generate_observations,generate_a_one_hot],axis=1)\n",
    "\n",
    "\n",
    "        len_fakes=dataset_gen.shape[0]\n",
    "\n",
    "        # Compilamos el discriminador como CNN\n",
    "        self.discriminator.discriminator_net.compile(optimizer=self.d_optimizer, loss=self.loss_fn_D, metrics=['accuracy'])\n",
    "\n",
    "        # Evaluamos como CNN\n",
    "        loss_real, acc_real=self.discriminator.discriminator_net.evaluate(X_test[0:len_real], tf.ones((len_real,1)), verbose=1)\n",
    "\n",
    "        loss_fake, acc_fake=self.discriminator.discriminator_net.evaluate(dataset_gen[0:len_fakes],tf.ones((len_fakes,1)), verbose=1)\n",
    "\n",
    "        print('>Loss real: ')\n",
    "        print(loss_real)\n",
    "        print('>Loss fake: ')\n",
    "        print(loss_fake)\n",
    "\n",
    "\n",
    "    # Evaluación del generador\n",
    "    def evaluate_G(self):\n",
    "        # Definimos el entorno\n",
    "        env= gym.make('CartPole-v1')\n",
    "\n",
    "        # Lista donde amacenaremos la recompensa acumulada de cada episodio.\n",
    "        # NUESTRO OBJETIVO: Agente aprenda a tomar las acciones que maximicen la recompensa\n",
    "        rewards=[]\n",
    "\n",
    "        # Para cada episodio, el Agente se mueve por el Entorno mediante acciones hasta llegar a un estado final\n",
    "        # siguiendo la política que se ha aprendido en el entrenamiento de la GAN\n",
    "        for episode in range(EPISODES_EVALUATE_G):\n",
    "            truncated=False\n",
    "            terminated=False\n",
    "            R=0.0\n",
    "            reward=0.0\n",
    "\n",
    "            # Estado inicial del juego\n",
    "            obs,_=env.reset()\n",
    "\n",
    "            #Interactuamos con el Entorno hasta que lleguemos a un estado final\n",
    "            while terminated!= True and truncated!=True:\n",
    "                action, _=self.generator.get_model().predict(obs)\n",
    "                obs,reward,terminated,info=env.step(int(action))\n",
    "\n",
    "                # Incremento la recompensa del episodio i al haber ejecutado el step\n",
    "                R+=reward\n",
    "\n",
    "            rewards.append(R)\n",
    "            # Vemos para el episodio, su recompensa acumulada que es lo que se trata de maximizar\n",
    "            print(\"Episode  {} Total reward: {}\".format(episode,R))\n",
    "\n",
    "        # Cierro el entorno\n",
    "        env.close()\n",
    "\n",
    "        # Muestro las recompensas obtenidas en cada episodio\n",
    "        indices = range(0, EPISODES_EVALUATE_G)\n",
    "        plt.plot(indices,rewards)\n",
    "        plt.show()\n",
    "\n",
    "    def train_step(self, X_train):\n",
    "        # Ojo no tenemos la misma cantidad de datos verdaderos y falsos, por eso calculamos len_real y len_fakes\n",
    "        # No podemos controlar la creación de x secuencias [s,a] ya que generaremos tantas secuencias como se\n",
    "        # necesiten para finalizar el juego\n",
    "\n",
    "        len_real = X_train.shape[0]\n",
    "\n",
    "        batch_size=len_real\n",
    "\n",
    "        generate_observations, generate_actions, rewards, Old_Policy, Policy=self.generator.generate_fakes(self.generator)\n",
    "\n",
    "        generate_a_one_hot=np.eye(env.action_space.n)[generate_actions]\n",
    "\n",
    "        if generate_observations.shape[0] == generate_a_one_hot.shape[0]:\n",
    "          dataset_gen = np.concatenate([generate_observations, generate_a_one_hot], axis=1)\n",
    "        else:\n",
    "          generate_a_one_hot_resized = np.resize(generate_a_one_hot, generate_observations.shape)\n",
    "          dataset_gen = np.concatenate([generate_observations, generate_a_one_hot_resized], axis=1)\n",
    "\n",
    "        len_fakes=dataset_gen.shape[0]\n",
    "\n",
    "        # Las etiquetas de las imagenes combinadas las tenemos que crear nosotros introduciendo algo de ruido\n",
    "        # con tf.random.uniform\n",
    "        labels = tf.concat(\n",
    "          [tf.ones((len_real, 1)), tf.zeros((len_fakes, 1))],\n",
    "          axis=0\n",
    "        )\n",
    "\n",
    "\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        if X_train.shape[1] == dataset_gen.shape[1]:\n",
    "            # Las dimensiones coinciden, se puede realizar la concatenación\n",
    "          combined_images = tf.concat([X_train, dataset_gen], axis=0)\n",
    "        else:\n",
    "          # Aquí debes realizar las modificaciones necesarias para asegurarte de que las dimensiones sean compatibles\n",
    "\n",
    "          # Por ejemplo, si quieres asegurarte de que ambas dimensiones 1 tengan el mismo tamaño,\n",
    "          # puedes ajustar el tamaño de dataset_gen para que coincida con el tamaño de X_train\n",
    "          dataset_gen_resized = dataset_gen[:, :X_train.shape[1]]  # Ajustar el tamaño al tamaño de X_train\n",
    "\n",
    "          # Luego, realizar la concatenación\n",
    "          combined_images = tf.concat([X_train, dataset_gen_resized], axis=0)\n",
    "\n",
    "        #############  PASO 1:  ENTRENAMIENTO DEL DISCRIMINADOR ##############################################\n",
    "\n",
    "        # Entrenamiento del discriminador con las [s,a] del agente y del experto combinadas, esto es,\n",
    "        # le pasamos un conjunto que tiene tanto imágenes reales como imágenes falsas\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions=np.zeros((len_real+len_fakes,6))\n",
    "            # Predicciones obtenidas con el discriminador\n",
    "            predictions = self.discriminator.discriminator_net(combined_images)\n",
    "            # Valor de la función de pérdida al comparar las predicciones con las etiquetas reales\n",
    "            d_loss = self.loss_fn_D(labels, predictions)\n",
    "\n",
    "        # Calculo del gradiente y actualización del gradiente\n",
    "        grads = tape.gradient(d_loss, self.discriminator.getNet().trainable_weights)\n",
    "\n",
    "        self.d_optimizer.apply_gradients(\n",
    "          zip(grads, self.discriminator.getNet().trainable_weights)\n",
    "        )\n",
    "\n",
    "\n",
    "        d_rewards = discriminator.getRewards()\n",
    "\n",
    "\n",
    "        ############# PASO 2: ENTRENAMIENTO DEL GENERADOR=POLÍTICA  ##############################\n",
    "\n",
    "        ppotrain=PPOTrain(Policy,Old_Policy,actions=generate_actions,rewards=rewards, obs=generate_observations[0])\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            g_loss = ppotrain.loss_fn_G()\n",
    "\n",
    "\n",
    "        g_loss = tf.cast(g_loss, dtype=tf.float32)\n",
    "\n",
    "\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "\n",
    "        return {\"d_loss\": self.d_loss_metric.result(),\n",
    "                    \"g_loss\": self.g_loss_metric.result()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfa28473"
   },
   "source": [
    "# Lectura de base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCarNjDkXTeb"
   },
   "source": [
    "## Base de datos experta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21763a85",
    "outputId": "c263c141-ab8c-405c-b44b-b3595b3e94a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEstados reales: \n",
      " [[-0.04456399  0.04653909  0.01326909 -0.02099827]\n",
      " [-0.04363321  0.24146827  0.01284913 -0.3094653 ]\n",
      " [-0.03880385  0.04616562  0.00665982 -0.01275795]\n",
      " ...\n",
      " [-0.69562376 -0.8861578  -0.07125074  0.6065587 ]\n",
      " [-0.71334696 -0.69011575 -0.05911956  0.29231167]\n",
      " [-0.72714925 -0.8843471  -0.05327333  0.56577873]]\n",
      "\tAcciones reales: \n",
      " [1 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "expert_observations = np.genfromtxt('/content/drive/MyDrive/Colab Notebooks/observations.csv')\n",
    "expert_actions = np.genfromtxt('/content/drive/MyDrive/Colab Notebooks/actions.csv', dtype=np.int32)\n",
    "\n",
    "print(\"\\n\\tEstados reales: \\n\", expert_observations)\n",
    "print(\"\\tAcciones reales: \\n\", expert_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49ac2e89",
    "outputId": "02809b9c-7309-4a0e-d788-5a0beb74b135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04456399  0.04653909  0.01326909 -0.02099827  0.          1.        ]\n",
      " [-0.04363321  0.24146827  0.01284913 -0.3094653   1.          0.        ]\n",
      " [-0.03880385  0.04616562  0.00665982 -0.01275795  1.          0.        ]\n",
      " ...\n",
      " [-0.69562376 -0.8861578  -0.07125074  0.6065587   0.          1.        ]\n",
      " [-0.71334696 -0.69011575 -0.05911956  0.29231167  1.          0.        ]\n",
      " [-0.72714925 -0.8843471  -0.05327333  0.56577873  0.          1.        ]]\n",
      "Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento: 3200\n",
      "Nº de (ESTADOS,ACCIONES) en el conjunto de prueba: 800\n"
     ]
    }
   ],
   "source": [
    "# Construimos el dataset [s,a] reales y lo dividimos en training y test\n",
    "expert_a_one_hot=np.eye(env.action_space.n)[expert_actions]\n",
    "\n",
    "dataset=np.concatenate([expert_observations,expert_a_one_hot],axis=1)\n",
    "\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#Imprime el número de elementos en el conjuntos de entrenamiento y prueba\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de entrenamiento:', len(X_train))\n",
    "print('Nº de (ESTADOS,ACCIONES) en el conjunto de prueba:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa3ae613"
   },
   "source": [
    "# Definición de generador, dicriminador y generamos [s,a]^*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "92e4545a"
   },
   "outputs": [],
   "source": [
    "env= gym.make('CartPole-v1')\n",
    "obs,_= env.reset()\n",
    "\n",
    "# Generador\n",
    "generator=Policy_net( 'policy', env, obs)\n",
    "\n",
    "# Generamos [s,a] falsas y las políticas theta_i y theta_i+1\n",
    "observations, actions, rewards, Old_Policy, Policy=generator.generate_fakes()\n",
    "\n",
    "# Discriminador\n",
    "discriminator=Discriminator(env, discriminator_net, expert_observations, expert_actions, observations, actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50f9e910"
   },
   "source": [
    "# EXPERIMENTACIÓN DE GAIL CON CARTPOLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWoOtQApX1sO"
   },
   "source": [
    "## Definición de GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "urrCBIpLHMKo"
   },
   "outputs": [],
   "source": [
    "gan=GAN(discriminator=discriminator,generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj6tieO1X41j"
   },
   "source": [
    "## Compilación de GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "feb5b0d5"
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss_fn_D=loss_fn_D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1q2qSu_YBDc"
   },
   "source": [
    "## Entrenamiento de GAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d917ecb9"
   },
   "source": [
    "En la ejecución de la primera época se puede ver como g_loss empieza a disminuir mientras d_loss empieza a aumentar,\n",
    "un comportamiento normal en las GANs ya que el generador produce muestras con intención de engañar al discriminador y lo consigue.\n",
    "\n",
    "Cuando el Discriminador aprenda los patrones con los que el generador esta generando las secuencias falsas, entonces\n",
    "el discriminador habra aprendido y su pérdida disminuirá mientras que g_loss incrementa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0e55429",
    "outputId": "6d27b192-ae32-4486-8388-803af38ed855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 43s 43s/step - d_loss: 0.6842 - g_loss: -1.0622\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.6812 - g_loss: -0.9625\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6959 - g_loss: -0.9207\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6442 - g_loss: -0.9828\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6925 - g_loss: -0.8683\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6277 - g_loss: -0.9230\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6229 - g_loss: -1.0562\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6220 - g_loss: -1.1885\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.6345 - g_loss: -0.9812\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6359 - g_loss: -1.1555\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6275 - g_loss: -0.8667\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6083 - g_loss: -0.9387\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.6106 - g_loss: -0.8732\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.5437 - g_loss: -1.0066\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.5473 - g_loss: -1.1290\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.5403 - g_loss: -0.9778\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.5248 - g_loss: -1.2380\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.5669 - g_loss: -0.9863\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.4928 - g_loss: -1.0331\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.5031 - g_loss: -0.8333\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.5162 - g_loss: -0.9452\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.4647 - g_loss: -0.9805\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.4945 - g_loss: -0.9888\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.4425 - g_loss: nan\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.4466 - g_loss: -1.2153\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.4637 - g_loss: -0.9729\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.4195 - g_loss: -1.0410\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.4259 - g_loss: -1.1264\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.3928 - g_loss: -0.9118\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.3993 - g_loss: -0.9244\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.3892 - g_loss: -0.9445\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.3807 - g_loss: -1.0067\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.3599 - g_loss: -0.8841\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.3431 - g_loss: -0.9577\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.3556 - g_loss: -0.9479\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.3288 - g_loss: -1.0034\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.4341 - g_loss: -0.9907\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.3053 - g_loss: -0.9267\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.2942 - g_loss: -0.7485\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.3034 - g_loss: -1.3736\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.3111 - g_loss: -1.0330\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.2745 - g_loss: -0.8335\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.2579 - g_loss: -1.1935\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.2595 - g_loss: -0.9952\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.2475 - g_loss: -1.1682\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.2603 - g_loss: -1.0169\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.2251 - g_loss: -0.8076\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.2199 - g_loss: -0.9948\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.2363 - g_loss: -1.0910\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1966 - g_loss: nan\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.2494 - g_loss: -0.8983\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1952 - g_loss: -1.0134\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1982 - g_loss: -1.0760\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1686 - g_loss: -1.3512\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1772 - g_loss: -0.9844\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1550 - g_loss: -1.3516\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1623 - g_loss: -1.1103\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1415 - g_loss: -1.0187\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1719 - g_loss: -0.9894\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1302 - g_loss: -0.7040\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1431 - g_loss: -0.9949\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1316 - g_loss: -0.9320\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1234 - g_loss: -1.1267\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1138 - g_loss: -0.9778\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.0974 - g_loss: nan\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1062 - g_loss: -1.3168\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1003 - g_loss: -1.1323\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1063 - g_loss: -0.7785\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1142 - g_loss: -0.8221\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0842 - g_loss: -1.0133\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0959 - g_loss: -0.7776\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0860 - g_loss: -0.8763\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0938 - g_loss: -0.9971\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0793 - g_loss: -1.0081\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0588 - g_loss: nan\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0653 - g_loss: -1.1468\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.1416 - g_loss: -1.1791\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0913 - g_loss: -1.1532\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0585 - g_loss: -1.1575\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.0731 - g_loss: -0.9692\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.0521 - g_loss: -1.2463\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.0888 - g_loss: -1.1377\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0424 - g_loss: -1.2994\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.0800 - g_loss: -1.0271\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.0369 - g_loss: -0.9098\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0800 - g_loss: -0.9610\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0629 - g_loss: -1.0857\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0448 - g_loss: -1.0587\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0420 - g_loss: -0.9415\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.0350 - g_loss: -1.0099\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.0544 - g_loss: -1.1570\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 42s 42s/step - d_loss: 0.0439 - g_loss: -1.0970\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0286 - g_loss: -1.0461\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0927 - g_loss: -0.9834\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0382 - g_loss: -0.8884\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0308 - g_loss: -0.8657\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0441 - g_loss: -1.0494\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0259 - g_loss: -0.9156\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0217 - g_loss: -0.9040\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 41s 41s/step - d_loss: 0.0740 - g_loss: -0.7663\n"
     ]
    }
   ],
   "source": [
    "# Deshabilitar los mensajes de información de TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Deshabilitar los mensajes de información de OpenAI Gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "\n",
    "history=gan.fit(X_train,\n",
    "    epochs=EPOCHS, batch_size=3200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0K3-f_cYFX0"
   },
   "source": [
    "## Evaluaación del Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6d7c1b7",
    "outputId": "33fbac12-7b71-407b-9ae4-cce90b46ba71",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0182 - accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0173 - accuracy: 0.0000e+00\n",
      ">Loss real: \n",
      "0.01818549819290638\n",
      ">Loss fake: \n",
      "0.017301257699728012\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el Discriminador de GAIL en el TEST\n",
    "gan.evaluate_D(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdah2vFDYTMR"
   },
   "source": [
    "## Evaluación del Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "id": "5ac18f6e",
    "outputId": "52de0de2-7c7a-4466-9680-f5c3c9569e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0 Total reward: 34.0\n",
      "Episode  1 Total reward: 28.0\n",
      "Episode  2 Total reward: 32.0\n",
      "Episode  3 Total reward: 17.0\n",
      "Episode  4 Total reward: 25.0\n",
      "Episode  5 Total reward: 14.0\n",
      "Episode  6 Total reward: 16.0\n",
      "Episode  7 Total reward: 35.0\n",
      "Episode  8 Total reward: 46.0\n",
      "Episode  9 Total reward: 37.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluamos el modelo GENERADOR en el TEST\n",
    "\n",
    "# Definimos el entorno\n",
    "env= gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "rewards=[]\n",
    "# Para cada episodio, el Agente se mueve por el Entorno mediante acciones hasta llegar a un estado final\n",
    "# siguiendo la política que se ha aprendido en el entrenamiento de la GAN\n",
    "for episode in range(10):\n",
    "    truncated=False\n",
    "    terminated=False\n",
    "    R=0.0\n",
    "    reward=0.0\n",
    "\n",
    "    # Estado inicial del juego\n",
    "    obs,_=env.reset()\n",
    "\n",
    "    #Interactuamos con el Entorno hasta que lleguemos a un estado final\n",
    "    while terminated!= True and truncated!=True:\n",
    "        action, _=generator.get_model().predict(obs)\n",
    "        obs,reward,terminated,truncated,info=env.step(int(action))\n",
    "\n",
    "        # Incremento la recompensa del episodio i al haber ejecutado el step\n",
    "        R+=reward\n",
    "\n",
    "    rewards.append(R)\n",
    "\n",
    "    # Vemos para el episodio, su recompensa acumulada que es lo que se trata de maximizar\n",
    "    print(\"Episode  {} Total reward: {}\".format(episode,R))\n",
    "\n",
    "# Cierro el entorno\n",
    "env.close()\n",
    "\n",
    "# Muestro las recompensas obtenidas en cada episodio\n",
    "indices = range(0, 10)\n",
    "plt.plot(indices,rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
